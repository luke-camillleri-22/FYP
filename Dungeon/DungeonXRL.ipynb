{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc12b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: DungeonShap_MultiAgent.py\n",
    "import numpy as np\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import onnxruntime as ort\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c455cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "BIN_PATHS = [\n",
    "    \"AgentObservations_Agent0.bin\",\n",
    "    \"AgentObservations_Agent1.bin\",\n",
    "    \"AgentObservations_Agent2.bin\"\n",
    "]\n",
    "ACTION_BIN_PATHS = [\n",
    "    \"AgentActions_Agent0.bin\",\n",
    "    \"AgentActions_Agent1.bin\",\n",
    "    \"AgentActions_Agent2.bin\"\n",
    "]\n",
    "SHAP_PATH = 'Dungeon_shap_agent'\n",
    "ONNX_PATH = \"Dungeon_with_logits.onnx\"\n",
    "OBS_SIZE = 410\n",
    "NUM_ACTIONS = 7\n",
    "RAY_FEATURES = 8\n",
    "RAYS_PER_SENSOR = 17\n",
    "NUM_SENSORS = 3\n",
    "RAY_TOTAL = RAYS_PER_SENSOR * NUM_SENSORS  # 51 rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79cf9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TAG DEFINITIONS ===\n",
    "TagLabels = [\"Wall\", \"Agent\", \"Dragon\", \"Key\", \"Lock\", \"Portal\"]\n",
    "SemanticGroups = {f\"{tag}\": [] for tag in TagLabels}\n",
    "SemanticGroups[\"Misses\"] = []\n",
    "SemanticGroups[\"Proximity\"] = []\n",
    "SemanticGroups[\"HasKey\"] = [408]\n",
    "SemanticGroups[\"DragonDead\"] = [409]\n",
    "\n",
    "for ray_i in range(RAY_TOTAL):\n",
    "    base = ray_i * RAY_FEATURES\n",
    "    for tag_i, tag in enumerate(TagLabels):\n",
    "        SemanticGroups[f\"{tag}\"].append(base + tag_i)\n",
    "    SemanticGroups[\"Misses\"].append(base + 6)\n",
    "    SemanticGroups[\"Proximity\"].append(base + 7)\n",
    "\n",
    "group_names = list(SemanticGroups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699105cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using logits output: /_discrete_distribution/Softmax_output_0\n"
     ]
    }
   ],
   "source": [
    "# === LOAD MODEL ===\n",
    "session = ort.InferenceSession(ONNX_PATH)\n",
    "input_names = [i.name for i in session.get_inputs()]\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "\n",
    "dummy_input = {\n",
    "    \"obs_0\": np.zeros((1, 408), dtype=np.float32),\n",
    "    \"obs_1\": np.zeros((1, 2), dtype=np.float32),\n",
    "    \"action_masks\": np.ones((1, NUM_ACTIONS), dtype=np.float32)\n",
    "}\n",
    "outputs = session.run(None, dummy_input)\n",
    "logits_name = [name for name, out in zip(output_names, outputs) if out.shape == (1, NUM_ACTIONS)][0]\n",
    "print(\"âœ… Using logits output:\", logits_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccea660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREDICT FUNCTION ===\n",
    "def predict_fn(X):\n",
    "    return session.run([logits_name], {\n",
    "        \"obs_0\": X[:, :408],\n",
    "        \"obs_1\": X[:, 408:],\n",
    "        \"action_masks\": np.ones((X.shape[0], NUM_ACTIONS), dtype=np.float32)\n",
    "    })[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bad5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action data 0 length: 9656\n",
      "Action data 1 length: 7247\n",
      "Action data 2 length: 7131\n",
      "Observation data 0 length: 9656.0\n",
      "Observation data 1 length: 7247.0\n",
      "Observation data 2 length: 7131.0\n"
     ]
    }
   ],
   "source": [
    "#load all action data and check lengths\n",
    "action_data = [np.fromfile(path, dtype=np.float32) for path in ACTION_BIN_PATHS]\n",
    "for i, data in enumerate(action_data):\n",
    "    print(f\"Action data {i} length: {len(data)}\")\n",
    "\n",
    "#load all observation data and check lengths\n",
    "obs_data = [np.fromfile(path, dtype=np.float32) for path in BIN_PATHS]\n",
    "for i, data in enumerate(obs_data):\n",
    "    print(f\"Observation data {i} length: {len(data)/410}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd95ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9656 valid observations from AgentObservations_Agent0.bin\n",
      "Loaded 9656 actions_taken from AgentActions_Agent0.bin\n",
      "ðŸŽ¯ Training surrogate for Action 0\n",
      "ðŸŽ¯ Training surrogate for Action 1\n",
      "ðŸŽ¯ Training surrogate for Action 2\n",
      "ðŸŽ¯ Training surrogate for Action 3\n",
      "ðŸŽ¯ Training surrogate for Action 4\n",
      "ðŸŽ¯ Training surrogate for Action 5\n",
      "ðŸŽ¯ Training surrogate for Action 6\n",
      "Loaded 7247 valid observations from AgentObservations_Agent1.bin\n",
      "Loaded 7247 actions_taken from AgentActions_Agent1.bin\n",
      "ðŸŽ¯ Training surrogate for Action 0\n",
      "ðŸŽ¯ Training surrogate for Action 1\n",
      "ðŸŽ¯ Training surrogate for Action 2\n",
      "ðŸŽ¯ Training surrogate for Action 3\n",
      "ðŸŽ¯ Training surrogate for Action 4\n",
      "ðŸŽ¯ Training surrogate for Action 5\n",
      "ðŸŽ¯ Training surrogate for Action 6\n",
      "Loaded 7131 valid observations from AgentObservations_Agent2.bin\n",
      "Loaded 7131 actions_taken from AgentActions_Agent2.bin\n",
      "ðŸŽ¯ Training surrogate for Action 0\n",
      "ðŸŽ¯ Training surrogate for Action 1\n",
      "ðŸŽ¯ Training surrogate for Action 2\n",
      "ðŸŽ¯ Training surrogate for Action 3\n",
      "ðŸŽ¯ Training surrogate for Action 4\n",
      "ðŸŽ¯ Training surrogate for Action 5\n",
      "ðŸŽ¯ Training surrogate for Action 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "semantic_shap_per_obs = [[],[],[]]\n",
    "\n",
    "for i, path in enumerate(BIN_PATHS):\n",
    "    data = np.fromfile(path, dtype=np.float32)\n",
    "    remainder = data.size % OBS_SIZE\n",
    "    if remainder != 0:\n",
    "        print(f\"Trimming {remainder} extra floats from {path}\")\n",
    "        data = data[:-remainder]\n",
    "    X = data.reshape(-1, OBS_SIZE)\n",
    "    X = X[np.isfinite(X).all(axis=1)]\n",
    "\n",
    "    print(f\"Loaded {X.shape[0]} valid observations from {path}\")\n",
    "\n",
    "    actions_taken = np.fromfile(ACTION_BIN_PATHS[i], dtype=np.int32)\n",
    "    if actions_taken.size != X.shape[0]:\n",
    "        print(f'num actions_taken ({actions_taken.size}) does not match num observations ({X.shape[0]})')\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Loaded {actions_taken.shape[0]} actions_taken from {ACTION_BIN_PATHS[i]}\")\n",
    "\n",
    "\n",
    "    Y = predict_fn(X)\n",
    "    Y = Y[np.isfinite(Y).all(axis=1)]\n",
    "    X = X[:len(Y)]  # Match length\n",
    "\n",
    "    for action_index in range(NUM_ACTIONS):\n",
    "        indices = np.where(actions_taken == action_index)[0]\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"ðŸŽ¯ Training surrogate for Action {action_index}\")\n",
    "        model = xgb.XGBRegressor(n_estimators=200, max_depth=15)\n",
    "        model.fit(X, Y[:, action_index])\n",
    "\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X[indices])\n",
    "\n",
    "        for obs_i, shap_val in zip(indices, shap_values.values):\n",
    "            sem_scores = {}\n",
    "            for group in group_names:\n",
    "                indices_in_group = SemanticGroups[group]\n",
    "                sem_scores[group] = float(np.abs(shap_val[indices_in_group]).mean())\n",
    "\n",
    "            semantic_shap_per_obs[i].append({\n",
    "                \"observation_index\": int(obs_i),\n",
    "                \"action\": int(action_index),\n",
    "                \"semantic_shap\": sem_scores\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79f7ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved semantic SHAP values to Dungeon_shap_agent\n",
      "âœ… Saved semantic SHAP values to Dungeon_shap_agent\n",
      "âœ… Saved semantic SHAP values to Dungeon_shap_agent\n"
     ]
    }
   ],
   "source": [
    "# === SAVE JSON ===\n",
    "for x in range(0, 3):\n",
    "    # === SAVE JSON ===\n",
    "    with open(f\"{SHAP_PATH}{x}.json\", \"w\") as f:\n",
    "        json.dump(semantic_shap_per_obs[x], f, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved semantic SHAP values to {SHAP_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
